{"cells":[{"cell_type":"markdown","source":["# Install the necessary packages"],"metadata":{"id":"PHOLGGnuKMfu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HibzRzS4j4qr"},"outputs":[],"source":["%%capture\n","!pip install unsloth\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n","!pip install triton\n","!pip install unsloth-zoo\n","!pip install -U xformers\n","!pip install -U bitsandbytes\n","\n","!pip install -q gradio"]},{"cell_type":"markdown","source":["# Import the necessary packages"],"metadata":{"id":"fBlAT2sMKRLK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RH-5A2CmiZES"},"outputs":[],"source":["import os\n","\n","from transformers import AutoModel, AutoTokenizer\n","from transformers import TextStreamer\n","from unsloth import FastLanguageModel\n","from unsloth.chat_templates import get_chat_template\n","\n","import random\n","import gradio as gr\n","from google.colab import drive"]},{"cell_type":"markdown","source":["### Deploy the UI on HuggingFace"],"metadata":{"id":"PlwMa5Y_C0MW"}},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"xeOCcZLIC2Ne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gradio deploy --share --auth \"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\""],"metadata":{"id":"hJqbHROJC3vY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load the model from HuggingFace, in 4-bit"],"metadata":{"id":"sEugqVJdKUj2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQ0AHzl3ivmv"},"outputs":[],"source":["max_seq_length = 2048\n","dtype = None\n","load_in_4bit = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcH5otxci8PY"},"outputs":[],"source":["model_name = \"celdot/lora_llama_model_4\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhaCXeIgjc_X"},"outputs":[],"source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = model_name,\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")"]},{"cell_type":"markdown","source":["# Test the prediction on 2 prompts :\n","\n","### *What is capitalism* and *Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,*\n","\n","### We use the code for inference that was in the template notebook to fine-tune the llama model.\n","### We test those promts with a smaller number of new tokens to generate, for the sake of speed."],"metadata":{"id":"UhyVBDxSKcFX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PTq1_EZi0Wy"},"outputs":[],"source":["def inference(message, model=model, tokenizer=tokenizer):\n","  tokenizer = get_chat_template(\n","      tokenizer,\n","      chat_template=\"llama-3.1\",\n","  )\n","\n","  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","  messages = [\n","      {\"role\": \"user\", \"content\": message},\n","  ]\n","\n","  inputs = tokenizer.apply_chat_template(\n","      messages,\n","      tokenize=True,\n","      add_generation_prompt=True,  # Must add for generation\n","      return_tensors=\"pt\",\n","  ).to(\"cuda\")\n","\n","  outputs = model.generate(\n","      input_ids=inputs,\n","      max_new_tokens=128,  # Adjust the number of new tokens to generate\n","      use_cache=True,\n","      temperature=1.5,   # Adjust temperature for randomness\n","      min_p=0.1,         # Adjust nucleus sampling probability\n","  )\n","\n","  generated_text = tokenizer.batch_decode(outputs)\n","\n","  return generated_text"]},{"cell_type":"code","source":["inference(\"What is capitalism\")"],"metadata":{"id":"cVCvvvbFM-QR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### We also use a `TextStreamer` for continuous inference - so we can see the generation token by token, instead of waiting the whole time"],"metadata":{"id":"UYxRgL-ILiP6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Herk4u6QkI-r"},"outputs":[],"source":["def stream_inference(message, model=model, tokenizer=tokenizer):\n","\n","  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","  messages = [\n","      {\"role\": \"user\", \"content\": message},\n","  ]\n","\n","  inputs = tokenizer.apply_chat_template(\n","      messages,\n","      tokenize = True,\n","      add_generation_prompt = True, # Must add for generation\n","      return_tensors = \"pt\",\n","  ).to(\"cuda\")\n","\n","\n","  text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n","  _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n","                    use_cache = True, temperature = 1.5, min_p = 0.1)"]},{"cell_type":"code","source":["stream_inference(\"What is capitalism\")"],"metadata":{"id":"HLQMm91WNlnO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inference(\"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\")"],"metadata":{"id":"_toEN4SjNrQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stream_inference(\"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\")"],"metadata":{"id":"SNVb0p7bNuJu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Set up the Chatbot UI with Gradio"],"metadata":{"id":"xIW83vvtNxJU"}},{"cell_type":"markdown","source":["### Define the ```respond``` function that defines how the chatbot is going to answer\n","\n","### We use the HuggingFace template to set up a chatbot, coupled with the previous functions to make inference."],"metadata":{"id":"vaoivjs4N1_H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBIeyGYdqks2"},"outputs":[],"source":["def respond(\n","    message,\n","    history: list[tuple[str, str]],\n","    system_message,\n","    max_tokens,\n","    temperature,\n","    top_p,\n","    tokenizer=tokenizer,\n","    model=model,\n","):\n","\n","    tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template=\"llama-3.1\",\n","    )\n","\n","    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","    # Construct the messages for the bot to analyze\n","    messages = [{\"role\": \"system\", \"content\": system_message}]\n","    for val in history:\n","        if val[\"role\"] == \"user\":\n","            messages.append({\"role\": \"user\", \"content\": val[\"content\"]})\n","        if val[\"role\"] == \"assistant\":\n","            messages.append({\"role\": \"assistant\", \"content\": val[\"content\"]})\n","    messages.append({\"role\": \"user\", \"content\": message})\n","\n","    inputs = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=True,\n","        add_generation_prompt=True,  # Must add for generation\n","        return_tensors=\"pt\",\n","    ).to(\"cuda\")\n","\n","    outputs = model.generate(\n","        input_ids = inputs,\n","        max_new_tokens=256,  # Adjust the number of new tokens to generate\n","        use_cache=True,\n","        temperature=1.5,   # Adjust temperature for randomness\n","        min_p=0.1,         # Adjust nucleus sampling probability\n","    )\n","\n","    generated_text = tokenizer.batch_decode(outputs\n","                                            , skip_special_tokens=True\n","                                            )\n","\n","    # Extract the bot's response\n","    full_text = generated_text[0]\n","    bot_response = full_text.split(\"assistant\\n\\n\")[-1]  # Remove the user message part\n","\n","    # Yield the response incrementally\n","    response = \"\"\n","    for token in bot_response:\n","        response += token\n","        yield response\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mToiR1TmSfqx"},"outputs":[],"source":["demo = gr.ChatInterface(\n","    respond,\n","    type='messages',\n","    additional_inputs=[\n","        gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),\n","        gr.Slider(minimum=1, maximum=2048, value=128, step=1, label=\"Max new tokens\"),\n","        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n","        gr.Slider(\n","            minimum=0.1,\n","            maximum=1.0,\n","            value=0.95,\n","            step=0.05,\n","            label=\"Top-p (nucleus sampling)\",\n","        ),\n","    ],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9eBr5WHShLs"},"outputs":[],"source":["demo.launch(share=True, debug=True)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}