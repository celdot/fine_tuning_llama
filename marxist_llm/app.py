# -*- coding: utf-8 -*-
"""marxist_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eVSEfEf-XlV-JY7Z8Ha1tIayzBUdRRTe

# Install the necessary packages
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
# !pip install triton
# !pip install unsloth-zoo
# !pip install -U xformers
# !pip install -U bitsandbytes
# 
# !pip install -q gradio

"""# Import the necessary packages"""

import os

from transformers import AutoModel, AutoTokenizer
from transformers import TextStreamer
from unsloth import FastLanguageModel
from unsloth.chat_templates import get_chat_template

import random
import gradio as gr

"""# Load the model from HuggingFace, in 4-bit"""

max_seq_length = 2048
dtype = None
load_in_4bit = True

model_name = "celdot/marxist-finetuned-llama"

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

"""# Test the prediction on 2 prompts :

### *What is capitalism* and *Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,*

### We use the code for inference that was in the template notebook to fine-tune the llama model.
### We test those promts with a smaller number of new tokens to generate, for the sake of speed.
"""

def inference(message, model=model, tokenizer=tokenizer):
  tokenizer = get_chat_template(
      tokenizer,
      chat_template="llama-3.1",
  )

  FastLanguageModel.for_inference(model) # Enable native 2x faster inference

  messages = [
      {"role": "user", "content": message},
  ]

  inputs = tokenizer.apply_chat_template(
      messages,
      tokenize=True,
      add_generation_prompt=True,  # Must add for generation
      return_tensors="pt",
  ).to("cuda")

  outputs = model.generate(
      input_ids=inputs,
      max_new_tokens=128,  # Adjust the number of new tokens to generate
      use_cache=True,
      temperature=1.5,   # Adjust temperature for randomness
      min_p=0.1,         # Adjust nucleus sampling probability
  )

  generated_text = tokenizer.batch_decode(outputs)

  return generated_text

inference("What is capitalism")

"""### We also use a `TextStreamer` for continuous inference - so we can see the generation token by token, instead of waiting the whole time"""

def stream_inference(message, model=model, tokenizer=tokenizer):

  FastLanguageModel.for_inference(model) # Enable native 2x faster inference

  messages = [
      {"role": "user", "content": message},
  ]

  inputs = tokenizer.apply_chat_template(
      messages,
      tokenize = True,
      add_generation_prompt = True, # Must add for generation
      return_tensors = "pt",
  ).to("cuda")


  text_streamer = TextStreamer(tokenizer, skip_prompt = True)
  _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,
                    use_cache = True, temperature = 1.5, min_p = 0.1)

stream_inference("What is capitalism")

inference("Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,")

stream_inference("Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,")

"""# Set up the Chatbot UI with Gradio

### Define the ```respond``` function that defines how the chatbot is going to answer

### We use the HuggingFace template to set up a chatbot, coupled with the previous functions to make inference.
"""

def respond(
    message,
    history: list[tuple[str, str]],
    system_message,
    max_tokens,
    temperature,
    top_p,
    tokenizer=tokenizer,
    model=model,
):

    tokenizer = get_chat_template(
    tokenizer,
    chat_template="llama-3.1",
    )

    FastLanguageModel.for_inference(model) # Enable native 2x faster inference

    # Construct the messages for the bot to analyze
    messages = [{"role": "system", "content": system_message}]
    for val in history:
        if val["role"] == "user":
            messages.append({"role": "user", "content": val["content"]})
        if val["role"] == "assistant":
            messages.append({"role": "assistant", "content": val["content"]})
    messages.append({"role": "user", "content": message})

    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,  # Must add for generation
        return_tensors="pt",
    ).to("cuda")

    outputs = model.generate(
        input_ids = inputs,
        max_new_tokens=256,  # Adjust the number of new tokens to generate
        use_cache=True,
        temperature=1.5,   # Adjust temperature for randomness
        min_p=0.1,         # Adjust nucleus sampling probability
    )

    generated_text = tokenizer.batch_decode(outputs
                                            , skip_special_tokens=True
                                            )

    # Extract the bot's response
    full_text = generated_text[0]
    bot_response = full_text.split("assistant\n\n")[-1]  # Remove the user message part

    # Yield the response incrementally
    response = ""
    for token in bot_response:
        response += token
        yield response

demo = gr.ChatInterface(
    respond,
    type='messages',
    additional_inputs=[
        gr.Textbox(value="You are a friendly Chatbot.", label="System message"),
        gr.Slider(minimum=1, maximum=2048, value=128, step=1, label="Max new tokens"),
        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label="Temperature"),
        gr.Slider(
            minimum=0.1,
            maximum=1.0,
            value=0.95,
            step=0.05,
            label="Top-p (nucleus sampling)",
        ),
    ],
)

demo.launch(share=True, debug=True)